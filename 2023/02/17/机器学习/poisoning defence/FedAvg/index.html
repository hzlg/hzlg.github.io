<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="机器学习,论文," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="联邦学习FedAvg写一下">
<meta property="og:type" content="article">
<meta property="og:title" content="FedAvg">
<meta property="og:url" content="https://hzlg.github.ioz/2023/02/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/poisoning%20defence/FedAvg/index.html">
<meta property="og:site_name" content="hzlg&#39;s blog">
<meta property="og:description" content="联邦学习FedAvg写一下">
<meta property="og:locale">
<meta property="og:image" content="https://hzlg.github.ioz/2023/02/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/poisoning%20defence/FedAvg/image-20230218142443182.png">
<meta property="og:image" content="https://hzlg.github.ioz/2023/02/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/poisoning%20defence/FedAvg/image-20230218142501464.png">
<meta property="article:published_time" content="2023-02-16T16:00:00.000Z">
<meta property="article:modified_time" content="2023-07-10T16:00:00.000Z">
<meta property="article:author" content="hzlg">
<meta property="article:tag" content="机器学习">
<meta property="article:tag" content="论文">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hzlg.github.ioz/2023/02/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/poisoning%20defence/FedAvg/image-20230218142443182.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://hzlg.github.ioz/2023/02/17/机器学习/poisoning defence/FedAvg/"/>





  <title>FedAvg | hzlg's blog</title>
  














<meta name="generator" content="Hexo 5.4.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">hzlg's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">笔记、日常</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://hzlg.github.ioz/2023/02/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/poisoning%20defence/FedAvg/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hzlg's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">FedAvg</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-02-17T00:00:00+08:00">
                2023-02-17
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2023-07-11T00:00:00+08:00">
                2023-07-11
              </time>
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>联邦学习FedAvg写一下</p>
<span id="more"></span>

<blockquote>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/a1105425455/article/details/117791839">机器学习入门（一）–MNIST（pytorch）模型的构造以及使用（详细）_亚伯拉罕·黄肯的博客-CSDN博客_mnist模型</a> </p>
<p>可视化:</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/flower1234568/article/details/114883137">【Pytorch+torchvision】MNIST手写数字识别（代码附最详细注释）_computer_Fleur的博客-CSDN博客</a></p>
<p>数据集处理:</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/HiWangWenBing/article/details/121055489">Pytorch系列-33\：数据集 - torchvision与MNIST数据集_文火冰糖的硅基工坊的博客-CSDN博客_torchvision mnist</a> </p>
<p>自建数据集getitem</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_37864815/article/details/109288921">pytorch\自己定义数据集Dataset_Kobin丶BRYANT的博客-CSDN博客</a> </p>
</blockquote>
<h2 id="加载库-参数定义-argprase"><a href="#加载库-参数定义-argprase" class="headerlink" title="加载库,参数定义 argprase"></a>加载库,参数定义 argprase</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets,transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line">BATCH_SIZE = <span class="number">64</span><span class="comment">#每批处理的数据 一次性多少个</span></span><br><span class="line">DEVICE  = torch.device(<span class="string">&quot;cuda&quot;</span>)<span class="comment">#使用GPU</span></span><br><span class="line">EPOCHS =<span class="number">10</span> <span class="comment">#训练数据集的轮次</span></span><br><span class="line"><span class="comment"># parser = argparse.ArgumentParser()</span></span><br><span class="line"><span class="comment"># dataset_list = [&quot;cifar10&quot;, &quot;mnist&quot;, &quot;famnist&quot;]</span></span><br><span class="line"><span class="comment"># parser.add_argument(&quot;--dataset&quot;, type=str, choices=dataset_list, default=&quot;cifar10&quot;)</span></span><br></pre></td></tr></table></figure>

<h2 id="数据加载-amp-处理-transform-datasets-dataloader"><a href="#数据加载-amp-处理-transform-datasets-dataloader" class="headerlink" title="数据加载&amp;处理 transform datasets dataloader"></a>数据加载&amp;处理 transform datasets dataloader</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">pipeline = transforms.Compose([transforms.ToTensor(), <span class="comment">#将图片转换为Tensor</span></span><br><span class="line">                               transforms.Normalize(mean = (<span class="number">0.1307</span>,),std = (<span class="number">0.3081</span>,))<span class="comment">#正则化 降低模型复杂度</span></span><br><span class="line">                               ])</span><br><span class="line">train_set = datasets.MNIST(<span class="string">&quot;data&quot;</span>,train=<span class="literal">True</span>,download=<span class="literal">True</span>,transform=pipeline)</span><br><span class="line">test_set = datasets.MNIST(<span class="string">&quot;data&quot;</span>,train=<span class="literal">False</span>,download=<span class="literal">True</span>,transform=pipeline)</span><br><span class="line"></span><br><span class="line"><span class="comment">#一次性加载BATCH_SIZE个打乱顺序的数据</span></span><br><span class="line">train_loader = DataLoader(train_set,batch_size=BATCH_SIZE,shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_set,batch_size=BATCH_SIZE,shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="网络定义-nn-Module-torchvision-datasets"><a href="#网络定义-nn-Module-torchvision-datasets" class="headerlink" title="网络定义: nn.Module /torchvision.datasets"></a>网络定义: nn.Module /torchvision.datasets</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">module</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()<span class="comment">#继承父类的函数</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>,<span class="number">10</span>,<span class="number">5</span>)<span class="comment"># 卷积 输入通道1，输出通道10，卷积核5（5*5）</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">10</span>,<span class="number">20</span>,<span class="number">3</span>)<span class="comment"># 10 20 3*3</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">20</span>*<span class="number">10</span>*<span class="number">10</span>,<span class="number">500</span>)<span class="comment">#全连接层</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">500</span>,<span class="number">10</span>)<span class="comment">#0到9十个数字 输出10</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span><span class="comment">#定义了forward函数，backward函数就会被自动实现(利用Autograd)</span></span><br><span class="line">        input_size = x.size(<span class="number">0</span>) <span class="comment"># batch_size *1 *28 *28</span></span><br><span class="line">        x = self.conv1(x) <span class="comment"># 输入 batch_size *1 *28 *28，输出 batch_size *10 *24 *24（28卷积5：28-5+1）</span></span><br><span class="line">        x = F.relu(x) <span class="comment">#激活函数 使其变为非线性函数</span></span><br><span class="line">        x = F.max_pool2d(x,<span class="number">2</span>,<span class="number">2</span>)<span class="comment">#池化层 保持shape不变 输出 batch_size *10 *12 *12（24/2）</span></span><br><span class="line"></span><br><span class="line">        x = self.conv2(x)<span class="comment">#输出： batch_size *20 *10 *10（12-3+1）</span></span><br><span class="line">        x = F.relu(x)<span class="comment">#激活函数</span></span><br><span class="line"></span><br><span class="line">        x = x.view(input_size,-<span class="number">1</span>)<span class="comment">#拉伸 -1(自动计算长度):20*10*10 = 2000</span></span><br><span class="line"></span><br><span class="line">        x = self.fc1(x)<span class="comment">#输入：2000 输出：500</span></span><br><span class="line">        x = F.relu(x)<span class="comment">#激活函数</span></span><br><span class="line">        x = self.fc2(x)<span class="comment">#输入：500 输出：10</span></span><br><span class="line"></span><br><span class="line">        output = F.log_softmax(x,dim=<span class="number">1</span>)<span class="comment">#计算分类后，每个数字的概率值</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="优化定义-optim"><a href="#优化定义-optim" class="headerlink" title="优化定义 optim"></a>优化定义 optim</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = module().to(DEVICE)<span class="comment">#创建模型并将模型加载到指定设备上</span></span><br><span class="line">optimizer = optim.Adam(model.parameters())<span class="comment">#优化函数</span></span><br></pre></td></tr></table></figure>

<h2 id="训练-train"><a href="#训练-train" class="headerlink" title="训练 train"></a>训练 train</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span>(<span class="params">model,device,train_loader,optimizer,epoch</span>):</span></span><br><span class="line">    model.train()<span class="comment">#模型训练</span></span><br><span class="line">    <span class="keyword">for</span> batch_index,(data ,target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):<span class="comment">#一批中的一个，（图片，标签）</span></span><br><span class="line">        data,target = data.to(device),target.to(device)<span class="comment">#部署到DEVICE上去</span></span><br><span class="line">        optimizer.zero_grad()<span class="comment">#梯度初始化为0</span></span><br><span class="line">        output = model(data)<span class="comment">#训练后的结果</span></span><br><span class="line">        loss = F.cross_entropy(output,target)<span class="comment">#多分类计算损失函数</span></span><br><span class="line">        loss.backward()<span class="comment">#反向传播 得到参数的梯度参数值</span></span><br><span class="line">        optimizer.step()<span class="comment">#参数优化</span></span><br><span class="line">        <span class="keyword">if</span> batch_index %<span class="number">3000</span> == <span class="number">0</span>:<span class="comment">#每3000个打印一次</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Train Epoch: &#123;&#125; \t Loss:&#123;:.6f&#125;&quot;</span>.<span class="built_in">format</span>(epoch,loss.item()))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="测试-test"><a href="#测试-test" class="headerlink" title="测试 test"></a>测试 test</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_model</span>(<span class="params">model,device,text_loader</span>):</span></span><br><span class="line">    model.<span class="built_in">eval</span>()<span class="comment">#模型验证</span></span><br><span class="line">    correct = <span class="number">0.0</span><span class="comment">#正确</span></span><br><span class="line">    Accuracy = <span class="number">0.0</span><span class="comment">#正确率</span></span><br><span class="line">    text_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():<span class="comment">#不会计算梯度，也不会进行反向传播</span></span><br><span class="line">        <span class="keyword">for</span> data,target <span class="keyword">in</span> text_loader:</span><br><span class="line">            data,target = data.to(device),target.to(device)<span class="comment">#部署到device上</span></span><br><span class="line">            output = model(data)<span class="comment">#处理后的结果</span></span><br><span class="line">            text_loss += F.cross_entropy(output,target).item()<span class="comment">#计算测试损失之和</span></span><br><span class="line">            pred = output.argmax(dim=<span class="number">1</span>)<span class="comment">#找到概率最大的下标（索引）</span></span><br><span class="line">            correct += pred.eq(target.view_as(pred)).<span class="built_in">sum</span>().item()<span class="comment">#累计正确的次数</span></span><br><span class="line">        text_loss /= <span class="built_in">len</span>(test_loader.dataset)<span class="comment">#损失和/数据集的总数量 = 平均loss</span></span><br><span class="line">        Accuracy = <span class="number">100.0</span>*correct / <span class="built_in">len</span>(text_loader.dataset)<span class="comment">#正确个数/数据集的总数量 = 正确率</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Test__Average loss: &#123;:4f&#125;,Accuracy: &#123;:.3f&#125;\n&quot;</span>.<span class="built_in">format</span>(text_loss,Accuracy))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="主函数"><a href="#主函数" class="headerlink" title="主函数"></a>主函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,EPOCHS+<span class="number">1</span>):</span><br><span class="line">    train_model(model,DEVICE,train_loader,optimizer,epoch)</span><br><span class="line">    test_model(model,DEVICE,test_loader)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="模型保存"><a href="#模型保存" class="headerlink" title="模型保存"></a>模型保存</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model.state_dict(),<span class="string">&#x27;model.ckpt&#x27;</span>)<span class="comment">#保存为model.ckpt</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="模型使用"><a href="#模型使用" class="headerlink" title="//模型使用"></a>//模型使用</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="keyword">as</span> cv</span><br><span class="line">src = cv.imread(<span class="string">&#x27;1.jpg&#x27;</span>)<span class="comment">#读取图片</span></span><br><span class="line">gray = cv.cvtColor(src, cv.COLOR_BGR2GRAY)</span><br><span class="line">ret, binary = cv.threshold(gray, <span class="number">0</span>, <span class="number">255</span>,cv.THRESH_BINARY_INV|cv.THRESH_OTSU)<span class="comment">#白底黑字转换为黑底白字</span></span><br><span class="line">cv.imwrite(<span class="string">&#x27;new1.jpg&#x27;</span>, binary)<span class="comment">#将图像数据写入到图像文件中</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h1 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.加载必要的库</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets,transforms</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.超参数</span></span><br><span class="line">BATCH_SIZE = <span class="number">32</span><span class="comment">#每批处理的数据 一次性多少个</span></span><br><span class="line">DEVICE = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)<span class="comment">#使用GPU</span></span><br><span class="line">EPOCHS =<span class="number">4</span> <span class="comment">#训练数据集的轮次</span></span><br><span class="line"><span class="comment"># 3.图像处理</span></span><br><span class="line">pipeline = transforms.Compose([transforms.ToTensor(), <span class="comment">#将图片转换为Tensor</span></span><br><span class="line">                               ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.下载，加载数据</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"> <span class="comment">#下载</span></span><br><span class="line">train_set = datasets.MNIST(<span class="string">&quot;data&quot;</span>,train=<span class="literal">True</span>,download=<span class="literal">True</span>,transform=pipeline)</span><br><span class="line">test_set = datasets.MNIST(<span class="string">&quot;data&quot;</span>,train=<span class="literal">False</span>,download=<span class="literal">True</span>,transform=pipeline)</span><br><span class="line"> <span class="comment">#加载 一次性加载BATCH_SIZE个打乱顺序的数据</span></span><br><span class="line">train_loader = DataLoader(train_set,batch_size=BATCH_SIZE,shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(test_set,batch_size=BATCH_SIZE,shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.构建网络模型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResBlk</span>(<span class="params">nn.Module</span>):</span>  <span class="comment"># 定义Resnet Block模块</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    resnet block</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ch_in, ch_out, stride=<span class="number">1</span></span>):</span>  <span class="comment"># 进入网络前先得知道传入层数和传出层数的设定</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param ch_in:</span></span><br><span class="line"><span class="string">        :param ch_out:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(ResBlk, self).__init__()  <span class="comment"># 初始化</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># we add stride support for resbok, which is distinct from tutorials.</span></span><br><span class="line">        <span class="comment"># 根据resnet网络结构构建2个（block）块结构 第一层卷积 卷积核大小3*3,步长为1，边缘加1</span></span><br><span class="line">        self.conv1 = nn.Conv2d(ch_in, ch_out, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 将第一层卷积处理的信息通过BatchNorm2d</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(ch_out)</span><br><span class="line">        <span class="comment"># 第二块卷积接收第一块的输出，操作一样</span></span><br><span class="line">        self.conv2 = nn.Conv2d(ch_out, ch_out, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(ch_out)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 确保输入维度等于输出维度</span></span><br><span class="line">        self.extra = nn.Sequential()  <span class="comment"># 先建一个空的extra</span></span><br><span class="line">        <span class="keyword">if</span> ch_out != ch_in:</span><br><span class="line">            <span class="comment"># [b, ch_in, h, w] =&gt; [b, ch_out, h, w]</span></span><br><span class="line">            self.extra = nn.Sequential(</span><br><span class="line">                nn.Conv2d(ch_in, ch_out, kernel_size=<span class="number">1</span>, stride=stride),</span><br><span class="line">                nn.BatchNorm2d(ch_out)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span>  <span class="comment"># 定义局部向前传播函数</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param x: [b, ch, h, w]</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        out = F.relu(self.bn1(self.conv1(x)))  <span class="comment"># 对第一块卷积后的数据再经过relu操作</span></span><br><span class="line">        out = self.bn2(self.conv2(out))  <span class="comment"># 第二块卷积后的数据输出</span></span><br><span class="line">        <span class="comment"># short cut.</span></span><br><span class="line">        <span class="comment"># extra module: [b, ch_in, h, w] =&gt; [b, ch_out, h, w]</span></span><br><span class="line">        <span class="comment"># element-wise add:</span></span><br><span class="line">        out = self.extra(x) + out  <span class="comment"># 将x传入extra经过2块（block）输出后与原始值进行相加</span></span><br><span class="line">        out = F.relu(out)  <span class="comment"># 调用relu，这里使用F.调用</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet18</span>(<span class="params">nn.Module</span>):</span>  <span class="comment"># 构建resnet18层</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(ResNet18, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Sequential(  <span class="comment"># 首先定义一个卷积层</span></span><br><span class="line">            nn.Conv2d(<span class="number">1</span>, <span class="number">32</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">3</span>, padding=<span class="number">0</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">32</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># followed 4 blocks 调用4次resnet网络结构，输出都是输入的2倍</span></span><br><span class="line">        <span class="comment"># [b, 64, h, w] =&gt; [b, 128, h ,w]</span></span><br><span class="line">        self.blk1 = ResBlk(<span class="number">32</span>, <span class="number">64</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># [b, 128, h, w] =&gt; [b, 256, h, w]</span></span><br><span class="line">        self.blk2 = ResBlk(<span class="number">64</span>, <span class="number">128</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># # [b, 256, h, w] =&gt; [b, 512, h, w]</span></span><br><span class="line">        self.blk3 = ResBlk(<span class="number">128</span>, <span class="number">256</span>, stride=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># # [b, 512, h, w] =&gt; [b, 1024, h, w]</span></span><br><span class="line">        self.blk4 = ResBlk(<span class="number">256</span>, <span class="number">256</span>, stride=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.outlayer = nn.Linear(<span class="number">256</span> * <span class="number">1</span> * <span class="number">1</span>, <span class="number">10</span>)  <span class="comment"># 最后是全连接层</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span>  <span class="comment"># 定义整个向前传播</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param x:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        x = F.relu(self.conv1(x))  <span class="comment"># 先经过第一层卷积</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># [b, 64, h, w] =&gt; [b, 1024, h, w]</span></span><br><span class="line">        x = self.blk1(x)  <span class="comment"># 然后通过4次resnet网络结构</span></span><br><span class="line">        x = self.blk2(x)</span><br><span class="line">        x = self.blk3(x)</span><br><span class="line">        x = self.blk4(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(&#x27;after conv:&#x27;, x.shape) #[b, 512, 2, 2]</span></span><br><span class="line">        <span class="comment"># F.adaptive_avg_pool2d功能尾巴变为1,1，[b, 512, h, w] =&gt; [b, 512, 1, 1]</span></span><br><span class="line">        x = F.adaptive_avg_pool2d(x, [<span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">        <span class="comment"># print(&#x27;after pool:&#x27;, x.shape)</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)  <span class="comment"># 平铺一维值</span></span><br><span class="line">        x = self.outlayer(x)  <span class="comment"># 全连接层</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"><span class="comment"># 6.定义优化器</span></span><br><span class="line">model = ResNet18().to(DEVICE)<span class="comment">#创建模型并将模型加载到指定设备上</span></span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(model.parameters(),lr=<span class="number">0.001</span>)<span class="comment">#优化函数</span></span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 7.训练</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_model</span>(<span class="params">model,device,train_loader,optimizer,epoch</span>):</span></span><br><span class="line">    <span class="comment"># Training settings</span></span><br><span class="line">    parser = argparse.ArgumentParser(description=<span class="string">&#x27;PyTorch MNIST Example&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--batch-size&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">64</span>, metavar=<span class="string">&#x27;N&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;input batch size for training (default: 64)&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--test-batch-size&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1000</span>, metavar=<span class="string">&#x27;N&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;input batch size for testing (default: 1000)&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--epochs&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">14</span>, metavar=<span class="string">&#x27;N&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;number of epochs to train (default: 14)&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--lr&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">1.0</span>, metavar=<span class="string">&#x27;LR&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;learning rate (default: 1.0)&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--gamma&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.7</span>, metavar=<span class="string">&#x27;M&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;Learning rate step gamma (default: 0.7)&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--no-cuda&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">False</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;disables CUDA training&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--dry-run&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">False</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;quickly check a single pass&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--seed&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1</span>, metavar=<span class="string">&#x27;S&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;random seed (default: 1)&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--log-interval&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">10</span>, metavar=<span class="string">&#x27;N&#x27;</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;how many batches to wait before logging training status&#x27;</span>)</span><br><span class="line">    parser.add_argument(<span class="string">&#x27;--save-model&#x27;</span>, action=<span class="string">&#x27;store_true&#x27;</span>, default=<span class="literal">False</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">&#x27;For Saving the current Model&#x27;</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    model.train()<span class="comment">#模型训练</span></span><br><span class="line">    <span class="keyword">for</span> batch_index,(data ,target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        data,target = data.to(device),target.to(device)<span class="comment">#部署到DEVICE上去</span></span><br><span class="line">        optimizer.zero_grad()<span class="comment">#梯度初始化为0</span></span><br><span class="line">        output = model(data)<span class="comment">#训练后的结果</span></span><br><span class="line">        loss = criterion(output,target)<span class="comment">#多分类计算损失</span></span><br><span class="line">        loss.backward()<span class="comment">#反向传播 得到参数的梯度值</span></span><br><span class="line">        optimizer.step()<span class="comment">#参数优化</span></span><br><span class="line">        <span class="keyword">if</span> batch_index % args.log_interval == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\tLoss: &#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                epoch, batch_index * <span class="built_in">len</span>(data), <span class="built_in">len</span>(train_loader.dataset),</span><br><span class="line">                       <span class="number">100.</span> * batch_index / <span class="built_in">len</span>(train_loader), loss.item()))</span><br><span class="line">            <span class="keyword">if</span> args.dry_run:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"><span class="comment"># 8.测试</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_model</span>(<span class="params">model,device,text_loader</span>):</span></span><br><span class="line">    model.<span class="built_in">eval</span>()<span class="comment">#模型验证</span></span><br><span class="line">    correct = <span class="number">0.0</span><span class="comment">#正确率</span></span><br><span class="line">    <span class="keyword">global</span> Accuracy</span><br><span class="line">    text_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():<span class="comment">#不会计算梯度，也不会进行反向传播</span></span><br><span class="line">        <span class="keyword">for</span> data,target <span class="keyword">in</span> text_loader:</span><br><span class="line">            data,target = data.to(device),target.to(device)<span class="comment">#部署到device上</span></span><br><span class="line">            output = model(data)<span class="comment">#处理后的结果</span></span><br><span class="line">            text_loss += criterion(output,target).item()<span class="comment">#计算测试损失</span></span><br><span class="line">            pred = output.argmax(dim=<span class="number">1</span>)<span class="comment">#找到概率最大的下标</span></span><br><span class="line">            correct += pred.eq(target.view_as(pred)).<span class="built_in">sum</span>().item()<span class="comment">#累计正确的值</span></span><br><span class="line">        text_loss /= <span class="built_in">len</span>(test_loader.dataset)<span class="comment">#损失和/加载的数据集的总数</span></span><br><span class="line">        Accuracy = <span class="number">100.0</span>*correct / <span class="built_in">len</span>(text_loader.dataset)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Test__Average loss: &#123;:4f&#125;,Accuracy: &#123;:.3f&#125;\n&quot;</span>.<span class="built_in">format</span>(text_loss,Accuracy))</span><br><span class="line"><span class="comment"># 9.调用</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,EPOCHS+<span class="number">1</span>):</span><br><span class="line">    train_model(model,DEVICE,train_loader,optimizer,epoch)</span><br><span class="line">    test_model(model,DEVICE,test_loader)</span><br><span class="line"></span><br><span class="line">torch.save(model.state_dict(),<span class="string">&#x27;model.ckpt&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="完整代码2-可视化"><a href="#完整代码2-可视化" class="headerlink" title="完整代码2(可视化)"></a>完整代码2(可视化)</h1><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/flower1234568/article/details/114883137">【Pytorch+torchvision】MNIST手写数字识别（代码附最详细注释）_computer_Fleur的博客-CSDN博客</a></p>
<blockquote>
<p><img src="/2023/02/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/poisoning%20defence/FedAvg/image-20230218142443182.png" alt="image-20230218142443182" style="zoom:67%;"> <img src="/2023/02/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/poisoning%20defence/FedAvg/image-20230218142501464.png" alt="image-20230218142501464" style="zoom:67%;"> </p>
<p>有bug,train只有一个epoch</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">train_loader = DataLoader(dataset=train_set,batch_size=64,shuffle=True)</span><br><span class="line">import os</span><br><span class="line">os.environ[&#x27;KMP_DUPLICATE_LIB_OK&#x27;] = &#x27;TRUE&#x27;</span><br><span class="line"># 实现单张图片可视化</span><br><span class="line">images, labels = next(iter(train_loader))</span><br><span class="line">datas = next(iter(train_loader))</span><br><span class="line">print(type(datas))</span><br><span class="line">print(datas)</span><br><span class="line">img = torchvision.utils.make_grid(images)</span><br><span class="line"></span><br><span class="line">img = img.numpy().transpose(1, 2, 0)</span><br><span class="line">std = [0.5, 0.5, 0.5]</span><br><span class="line">mean = [0.5, 0.5, 0.5]</span><br><span class="line">img = img * std + mean</span><br><span class="line">print(labels)</span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn <span class="comment">#torch.nn层中包含可训练的参数</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment">#注意下面两行在matplotlib使用上出错时，加上可不出错</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>] = <span class="string">&#x27;TRUE&#x27;</span></span><br><span class="line"></span><br><span class="line">n_epochs = <span class="number">3</span> <span class="comment">#epoch的数量定义了将循环整个训练数据集的次数</span></span><br><span class="line">batch_size_train = <span class="number">64</span> <span class="comment">#每次投喂的样本数量</span></span><br><span class="line">batch_size_test = <span class="number">1000</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">momentum = <span class="number">0.5</span> <span class="comment">#优化器的超参数</span></span><br><span class="line">log_interval = <span class="number">10</span></span><br><span class="line">random_seed = <span class="number">1</span></span><br><span class="line">torch.manual_seed(random_seed) <span class="comment">#对于可重复的实验，须为任何使用随机数产生的东西设置随机种子</span></span><br><span class="line"><span class="comment">#训练集数据</span></span><br><span class="line">train_loader = torch.utils.data.DataLoader(</span><br><span class="line">  torchvision.datasets.MNIST(<span class="string">&#x27;./data/&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, <span class="comment">#加载该数据集(download=True)</span></span><br><span class="line">                             transform=torchvision.transforms.Compose([</span><br><span class="line">                               torchvision.transforms.ToTensor(),</span><br><span class="line">                               torchvision.transforms.Normalize(</span><br><span class="line">                                 (<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">                             ])), <span class="comment">#Normalize()转换使用的值0.1307和0.3081是该数据集的全局平均值和标准偏差，这里将它们作为给定值</span></span><br><span class="line">  batch_size=batch_size_train, shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#测试集数据</span></span><br><span class="line">test_loader = torch.utils.data.DataLoader(</span><br><span class="line">  torchvision.datasets.MNIST(<span class="string">&#x27;./data/&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>,</span><br><span class="line">                             transform=torchvision.transforms.Compose([</span><br><span class="line">                               torchvision.transforms.ToTensor(),</span><br><span class="line">                               torchvision.transforms.Normalize(</span><br><span class="line">                                 (<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">                             ])),</span><br><span class="line">  batch_size=batch_size_test, shuffle=<span class="literal">True</span>) <span class="comment">#使用size=1000对这个数据集进行测试</span></span><br><span class="line"><span class="comment">#查看一批测试数据由什么组成</span></span><br><span class="line">examples = <span class="built_in">enumerate</span>(test_loader) <span class="comment">#enumerate指循环，类似for</span></span><br><span class="line">batch_idx, (example_data, example_targets) = <span class="built_in">next</span>(examples) <span class="comment">#example_targets是图片实际对应的数字标签，example_data是指图片本身数据</span></span><br><span class="line"><span class="built_in">print</span>(example_targets)</span><br><span class="line"><span class="built_in">print</span>(example_data.shape) <span class="comment">#输出torch.Size([1000, 1, 28, 28])，意味着我们有1000个例子的28x28像素的灰度(即没有rgb通道)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义卷积神经网络</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        <span class="comment"># batch*1*28*28（每次会送入batch个样本，输入通道数1（黑白图像），图像分辨率是28x28）</span></span><br><span class="line">        <span class="comment"># 下面的卷积层Conv2d的第一个参数指输入通道数，第二个参数指输出通道数（即用了几个卷积核），第三个参数指卷积核的大小</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>) <span class="comment">#因为图像为黑白的，所以输入通道为1,此时输出数据大小变为28-5+1=24.所以batchx1x28x28 -&gt; batchx10x24x24</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>) <span class="comment">#第一个卷积层的输出通道数等于第二个卷积层是输入通道数。</span></span><br><span class="line">        self.conv2_drop = nn.Dropout2d() <span class="comment">#在前向传播时，让某个神经元的激活值以一定的概率p停止工作，可以使模型泛化性更强，因为它不会太依赖某些局部的特征</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">320</span>, <span class="number">50</span>) <span class="comment">#由于下部分前向传播处理后，输出数据为20x4x4=320，传递给全连接层。# 输入通道数是320，输出通道数是50</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">50</span>, <span class="number">10</span>)<span class="comment">#输入通道数是50，输出通道数是10，（即10分类（数字1-9），最后结果需要分类为几个就是几个输出通道数）。全连接层（Linear）：y=x乘A的转置+b</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv1(x), <span class="number">2</span>)) <span class="comment"># batch*10*24*24 -&gt; batch*10*12*12（2*2的池化层会减半，步长为2）（激活函数ReLU不改变形状）</span></span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), <span class="number">2</span>)) <span class="comment">#此时输出数据大小变为12-5+1=8（卷积核大小为5）（2*2的池化层会减半）。所以 batchx10x12x12 -&gt; batchx20x4x4。</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">320</span>) <span class="comment"># batch*20*4*4 -&gt; batch*320</span></span><br><span class="line">        x = F.relu(self.fc1(x)) <span class="comment">#进入全连接层</span></span><br><span class="line">        x = F.dropout(x, training=self.training) <span class="comment">#减少遇到过拟合问题，dropout层是一个很好的规范模型。</span></span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="comment">#计算log(softmax(x))</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x)</span><br><span class="line"><span class="comment">#初始化网络和优化器</span></span><br><span class="line"><span class="comment">#如果我们使用GPU进行训练，应使用例如network.cuda()将网络参数发送给GPU。将网络参数传递给优化器之前，将它们传输到适当的设备很重要，否则优化器无法以正确的方式跟踪它们。</span></span><br><span class="line">network = Net()</span><br><span class="line">optimizer = optim.SGD(network.parameters(), lr=learning_rate,</span><br><span class="line">                      momentum=momentum)</span><br><span class="line">train_losses = []</span><br><span class="line">train_counter = []</span><br><span class="line">test_losses = []</span><br><span class="line">test_counter = [i*<span class="built_in">len</span>(train_loader.dataset) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs + <span class="number">1</span>)]</span><br><span class="line"><span class="comment">#每个epoch对所有训练数据进行一次迭代。加载单独批次由DataLoader处理</span></span><br><span class="line"><span class="comment">#训练函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">epoch</span>):</span></span><br><span class="line">    network.train() <span class="comment">#在训练模型时会在前面加上</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        optimizer.zero_grad() <span class="comment">#使用optimizer.zero_grad()手动将梯度设置为零，因为PyTorch在默认情况下会累积梯度</span></span><br><span class="line">        output = network(data) <span class="comment">#生成网络的输出(前向传递)</span></span><br><span class="line">        loss = F.nll_loss(output, target) <span class="comment">#计算输出（output）与真值标签（target）之间的负对数概率损失</span></span><br><span class="line">        loss.backward() <span class="comment">#对损失反向传播</span></span><br><span class="line">        optimizer.step() <span class="comment">#收集一组新的梯度，并使用optimizer.step()将其传播回每个网络参数</span></span><br><span class="line">        <span class="keyword">if</span> batch_idx % log_interval == <span class="number">0</span>: <span class="comment">#log_interval=10，每10次投喂后输出一次</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\tLoss: &#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">                epoch, batch_idx * <span class="built_in">len</span>(data), <span class="built_in">len</span>(train_loader.dataset),</span><br><span class="line">                       <span class="number">100.</span> * batch_idx / <span class="built_in">len</span>(train_loader), loss.item()))</span><br><span class="line">            train_losses.append(loss.item()) <span class="comment">#添加进训练损失列表中</span></span><br><span class="line">            train_counter.append(</span><br><span class="line">                (batch_idx * <span class="number">64</span>) + ((epoch - <span class="number">1</span>) * <span class="built_in">len</span>(train_loader.dataset)))</span><br><span class="line">            <span class="comment">#神经网络模块以及优化器能够使用.state_dict()保存和加载它们的内部状态。这样，如果需要，我们就可以继续从以前保存的状态dict中进行训练——只需调用.load_state_dict(state_dict)。</span></span><br><span class="line">            torch.save(network.state_dict(), <span class="string">&#x27;./model.pth&#x27;</span>)</span><br><span class="line">            torch.save(optimizer.state_dict(), <span class="string">&#x27;./optimizer.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#测试函数。总结测试损失，并跟踪正确分类的数字来计算网络的精度。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>():</span></span><br><span class="line">    network.<span class="built_in">eval</span>() <span class="comment">#在测试模型时在前面使用</span></span><br><span class="line">    test_loss = <span class="number">0</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad(): <span class="comment">#使用上下文管理器no_grad()，我们可以避免将生成网络输出的计算结果存储在计算图（计算过程的构建，以便梯度反向传播等操作）中。（with是使用的意思）</span></span><br><span class="line">        <span class="keyword">for</span> data, target <span class="keyword">in</span> test_loader:</span><br><span class="line">            output = network(data) <span class="comment">#生成网络的输出(前向传递)</span></span><br><span class="line">            <span class="comment"># 将一批的损失相加</span></span><br><span class="line">            test_loss += F.nll_loss(output, target, size_average=<span class="literal">False</span>).item() <span class="comment">#NLLLoss 的输入是一个对数概率向量和一个目标标签</span></span><br><span class="line">            pred = output.data.<span class="built_in">max</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>] <span class="comment">## 找到概率最大的下标</span></span><br><span class="line">            correct += pred.eq(target.data.view_as(pred)).<span class="built_in">sum</span>() <span class="comment">#预测正确的数量相加</span></span><br><span class="line">    test_loss /= <span class="built_in">len</span>(test_loader.dataset)</span><br><span class="line">    test_losses.append(test_loss)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\nTest set: Avg. loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\n&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">        test_loss, correct, <span class="built_in">len</span>(test_loader.dataset),</span><br><span class="line">        <span class="number">100.</span> * correct / <span class="built_in">len</span>(test_loader.dataset)))</span><br><span class="line"></span><br><span class="line">test()</span><br><span class="line"></span><br><span class="line"><span class="comment">#我们将在循环遍历n_epochs之前手动添加test()调用，以使用随机初始化的参数来评估我们的模型。</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n_epochs + <span class="number">1</span>):</span><br><span class="line">  train(epoch)</span><br><span class="line">  test()</span><br><span class="line"></span><br><span class="line"><span class="comment">#评估模型的性能，画损失曲线</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">plt.plot(train_counter, train_losses, color=<span class="string">&#x27;blue&#x27;</span>)</span><br><span class="line">plt.scatter(test_counter, test_losses, color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">plt.legend([<span class="string">&#x27;Train Loss&#x27;</span>, <span class="string">&#x27;Test Loss&#x27;</span>], loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;number of training examples seen&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;negative log likelihood loss&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出自己找的测试图片，比较模型的输出。</span></span><br><span class="line">examples = <span class="built_in">enumerate</span>(test_loader)</span><br><span class="line">batch_idx, (example_data, example_targets) = <span class="built_in">next</span>(examples)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  output = network(example_data)</span><br><span class="line">fig1 = plt.figure()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">  plt.subplot(<span class="number">2</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">  plt.tight_layout()</span><br><span class="line">  plt.imshow(example_data[i][<span class="number">0</span>], cmap=<span class="string">&#x27;gray&#x27;</span>, interpolation=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">  plt.title(<span class="string">&quot;Prediction: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">    output.data.<span class="built_in">max</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>][i].item()))</span><br><span class="line">  plt.xticks([])</span><br><span class="line">  plt.yticks([])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#继续对网络进行训练，并看看如何从第一次培训运行时保存的state_dicts中继续进行训练。我们将初始化一组新的网络和优化器。</span></span><br><span class="line">continued_network = Net()</span><br><span class="line">continued_optimizer = optim.SGD(network.parameters(), lr=learning_rate,</span><br><span class="line">                                momentum=momentum)</span><br><span class="line"></span><br><span class="line">network_state_dict = torch.load(<span class="string">&#x27;model.pth&#x27;</span>) <span class="comment">#见左侧项目列表，有该文件</span></span><br><span class="line">continued_network.load_state_dict(network_state_dict) <span class="comment">#使用.load_state_dict()，我们现在可以加载网络的内部状态，并在最后一次保存它们时优化它们。</span></span><br><span class="line">optimizer_state_dict = torch.load(<span class="string">&#x27;optimizer.pth&#x27;</span>) <span class="comment">#见左侧项目列表，有该文件</span></span><br><span class="line">continued_optimizer.load_state_dict(optimizer_state_dict)</span><br><span class="line"><span class="comment">#同样，运行一个训练循环应该立即恢复我们之前的训练。为了检查这一点，我们只需使用与前面相同的列表来跟踪损失值</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>,<span class="number">9</span>):</span><br><span class="line">  test_counter.append(i*<span class="built_in">len</span>(train_loader.dataset))</span><br><span class="line">  train(i)</span><br><span class="line">  test()</span><br><span class="line"><span class="comment">#我们再次看到测试集的准确性从一个epoch到另一个epoch有了(运行更慢的，慢的多了)提高。</span></span><br><span class="line"><span class="comment">#输出自己找的测试图片，比较模型的输出。</span></span><br><span class="line">examples = <span class="built_in">enumerate</span>(test_loader)</span><br><span class="line">batch_idx, (example_data, example_targets) = <span class="built_in">next</span>(examples)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  output = network(example_data)</span><br><span class="line">fig1 = plt.figure()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">  plt.subplot(<span class="number">2</span>,<span class="number">3</span>,i+<span class="number">1</span>)</span><br><span class="line">  plt.tight_layout()</span><br><span class="line">  plt.imshow(example_data[i][<span class="number">0</span>], cmap=<span class="string">&#x27;gray&#x27;</span>, interpolation=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">  plt.title(<span class="string">&quot;Prediction: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(</span><br><span class="line">    output.data.<span class="built_in">max</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>][i].item()))</span><br><span class="line">  plt.xticks([])</span><br><span class="line">  plt.yticks([])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="联邦代码"><a href="#联邦代码" class="headerlink" title="联邦代码"></a>联邦代码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets,transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> data</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">&quot;--datadir&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=<span class="string">&quot;./data/&quot;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--batchsize&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">64</span>)</span><br><span class="line">parser.add_argument(<span class="string">&quot;--device&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, default=torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>))</span><br><span class="line">parser.add_argument(<span class="string">&quot;--train_local_learnrate&quot;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">0.01</span>)</span><br><span class="line">datasets_list = [<span class="string">&quot;cifar10&quot;</span>, <span class="string">&quot;mnist&quot;</span>, <span class="string">&quot;famnist&quot;</span>]</span><br><span class="line">parser.add_argument(<span class="string">&quot;--dataset&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, choices=datasets_list, default=<span class="string">&quot;cifar10&quot;</span>)</span><br><span class="line"></span><br><span class="line">models = [<span class="string">&quot;resnet18_cha&quot;</span>, <span class="string">&quot;resnet18_train&quot;</span>, <span class="string">&quot;tv_resnet18&quot;</span>, <span class="string">&quot;resnet9&quot;</span>, <span class="string">&quot;lenet&quot;</span>, <span class="string">&quot;vgg16&quot;</span>]</span><br><span class="line">parser.add_argument(<span class="string">&quot;--model&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, choices=models, default=<span class="string">&quot;resnet18_train&quot;</span>)</span><br><span class="line"></span><br><span class="line">partitions = [<span class="string">f&quot;noniid<span class="subst">&#123;i&#125;</span>&quot;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>)] <span class="comment">#noniidX每个客户端至多有X类  1-10</span></span><br><span class="line">partitions += [<span class="string">f&quot;avenoniid<span class="subst">&#123;i&#125;</span>&quot;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">11</span>)] <span class="comment">#noniidX每个客户端至多有X类 1-10</span></span><br><span class="line">partitions += [<span class="string">&quot;IID&quot;</span>, <span class="string">&quot;ALL&quot;</span>, <span class="string">&quot;manual1&quot;</span>]    <span class="comment">#IID全部随机</span></span><br><span class="line">parser.add_argument(<span class="string">&quot;--partition&quot;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>, choices=partitions, default=<span class="string">&quot;manual2&quot;</span>)  <span class="comment">#数据划分方式</span></span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">&quot;--num_clients&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">5</span>)           <span class="comment">#联邦用户数</span></span><br><span class="line">parser.add_argument(<span class="string">&quot;--work_frac&quot;</span>, <span class="built_in">type</span>=<span class="built_in">float</span>, default=<span class="number">1.0</span>)         <span class="comment">#每轮参与的比例</span></span><br><span class="line">parser.add_argument(<span class="string">&quot;--epoch&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">5</span>)                 <span class="comment">#联邦轮数</span></span><br><span class="line">parser.add_argument(<span class="string">&quot;--nsamples_shared&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">500</span>)      <span class="comment">#共享数据样本数 每一类  20*10类个</span></span><br><span class="line">parser.add_argument(<span class="string">&quot;--train_local_batchsize&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">parser.add_argument(<span class="string">&quot;--train_local_epochs&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">2</span>)        <span class="comment">#本地训练轮数</span></span><br><span class="line">parser.add_argument(<span class="string">&quot;--num_classes&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">10</span>)      <span class="comment">#数据集的类数</span></span><br><span class="line">parser.add_argument(<span class="string">&quot;--in_channels&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">1</span>)      <span class="comment">#网络输入channel数</span></span><br><span class="line">parser.add_argument(<span class="string">&quot;--seed&quot;</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">2022</span>)</span><br><span class="line"></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">np.random.seed(args.seed)</span><br><span class="line">random.seed(args.seed)</span><br><span class="line">torch.manual_seed(args.seed)</span><br><span class="line">torch.cuda.manual_seed(args.seed)</span><br><span class="line">torch.cuda.manual_seed_all(args.seed)</span><br><span class="line">np.set_printoptions(precision=<span class="number">3</span>) <span class="comment"># 输出小数点后三位</span></span><br><span class="line">torch.set_printoptions(precision=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net, self).__init__()</span><br><span class="line">        <span class="comment"># batch*1*28*28（每次会送入batch个样本，输入通道数1（黑白图像），图像分辨率是28x28）</span></span><br><span class="line">        <span class="comment"># 下面的卷积层Conv2d的第一个参数指输入通道数，第二个参数指输出通道数（即用了几个卷积核），第三个参数指卷积核的大小</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>) <span class="comment">#因为图像为黑白的，所以输入通道为1,此时输出数据大小变为28-5+1=24.所以batchx1x28x28 -&gt; batchx10x24x24</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>) <span class="comment">#第一个卷积层的输出通道数等于第二个卷积层是输入通道数。</span></span><br><span class="line">        self.conv2_drop = nn.Dropout2d() <span class="comment">#在前向传播时，让某个神经元的激活值以一定的概率p停止工作，可以使模型泛化性更强，因为它不会太依赖某些局部的特征</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">320</span>, <span class="number">50</span>) <span class="comment">#由于下部分前向传播处理后，输出数据为20x4x4=320，传递给全连接层。# 输入通道数是320，输出通道数是50</span></span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">50</span>, <span class="number">10</span>)<span class="comment">#输入通道数是50，输出通道数是10，（即10分类（数字1-9），最后结果需要分类为几个就是几个输出通道数）。全连接层（Linear）：y=x乘A的转置+b</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv1(x), <span class="number">2</span>)) <span class="comment"># batch*10*24*24 -&gt; batch*10*12*12（2*2的池化层会减半，步长为2）（激活函数ReLU不改变形状）</span></span><br><span class="line">        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), <span class="number">2</span>)) <span class="comment">#此时输出数据大小变为12-5+1=8（卷积核大小为5）（2*2的池化层会减半）。所以 batchx10x12x12 -&gt; batchx20x4x4。</span></span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">320</span>) <span class="comment"># batch*20*4*4 -&gt; batch*320</span></span><br><span class="line">        x = F.relu(self.fc1(x)) <span class="comment">#进入全连接层</span></span><br><span class="line">        x = F.dropout(x, training=self.training) <span class="comment">#减少遇到过拟合问题，dropout层是一个很好的规范模型。</span></span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="comment">#计算log(softmax(x))</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net_glob = Net()</span><br><span class="line">users_model = []</span><br><span class="line"><span class="keyword">for</span> net_i <span class="keyword">in</span> <span class="built_in">range</span>(-<span class="number">1</span>, args.num_clients):</span><br><span class="line">    users_model.append(copy.deepcopy(net_glob))</span><br><span class="line"></span><br><span class="line">transformer=transforms.Compose([transforms.ToTensor(),transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))])</span><br><span class="line">train_set = datasets.MNIST(root=args.datadir,train=<span class="literal">True</span>,download=<span class="literal">True</span>,transform=transformer)</span><br><span class="line">test_set = datasets.MNIST(root=args.datadir,train=<span class="literal">False</span>,download=<span class="literal">True</span>,transform=transformer)</span><br><span class="line"></span><br><span class="line">x_train, y_train = train_set.data, np.array(train_set.targets)</span><br><span class="line">n_train = y_train.shape[<span class="number">0</span>]          <span class="comment">#训练集样本数量</span></span><br><span class="line"><span class="comment"># IID</span></span><br><span class="line">idxs = np.random.permutation(n_train) <span class="comment"># 将样本数量随机排序</span></span><br><span class="line">batch_idxs = np.array_split(idxs, args.num_clients) <span class="comment"># 分成20(客户端数量)个数组</span></span><br><span class="line">net_dataidx_map = &#123;i: batch_idxs[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(args.num_clients)&#125;</span><br><span class="line"><span class="comment">#ALL</span></span><br><span class="line"><span class="comment"># net_dataidx_map = &#123;i: [_ for _ in range(n_train)] for i in range(args.num_clients)&#125;</span></span><br><span class="line"></span><br><span class="line">net_cls_counts = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> net_i, dataidx <span class="keyword">in</span> net_dataidx_map.items():</span><br><span class="line">    unq, unq_cnt = np.unique(y_train[dataidx], return_counts=<span class="literal">True</span>) <span class="comment"># 去除重复元素，得到类名和该类元素数量</span></span><br><span class="line">    tmp = &#123;unq[i]: unq_cnt[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(unq))&#125; <span class="comment"># 改为字典形式</span></span><br><span class="line">    net_cls_counts[net_i] = tmp</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Data statistics Train:\n \t %s&#x27;</span> % <span class="built_in">str</span>(net_cls_counts))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MNIST_truncated</span>(<span class="params">data.Dataset</span>):</span> <span class="comment"># 根据序号参数切分mnist</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, root, dataidxs=<span class="literal">None</span>, train=<span class="literal">True</span>, transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span>, download=<span class="literal">False</span></span>):</span></span><br><span class="line">        self.root = root</span><br><span class="line">        self.dataidxs = dataidxs</span><br><span class="line">        self.train = train</span><br><span class="line">        self.transform = transform</span><br><span class="line">        self.target_transform = target_transform</span><br><span class="line">        self.download = download</span><br><span class="line">        self.data, self.targets = self.__build_truncated_dataset__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__build_truncated_dataset__</span>(<span class="params">self</span>):</span></span><br><span class="line">        mnist_dataobj = datasets.MNIST(self.root, self.train, self.transform, self.target_transform, self.download)</span><br><span class="line">        data = mnist_dataobj.data</span><br><span class="line">        target = np.array(mnist_dataobj.targets)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.dataidxs <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment"># 给了序号就只用这部分data</span></span><br><span class="line">            data = data[self.dataidxs]</span><br><span class="line">            target = target[self.dataidxs]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> data, target</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, index</span>):</span><span class="comment">#index是下标</span></span><br><span class="line">        img, target = self.data[index],<span class="built_in">int</span>(self.targets[index])<span class="comment">#返回对应下标的数据和标签（size为[28,28],标签是个一维tensor）[需要]</span></span><br><span class="line"></span><br><span class="line">        img = Image.fromarray(img.numpy(), mode=<span class="string">&#x27;L&#x27;</span>)<span class="comment">#将tensor类型的img转成numpy类型，然后再转成PIL Image 类型[需要]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:<span class="comment">#将img进行一些变换[需要]</span></span><br><span class="line">            img = self.transform(img)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.target_transform <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:<span class="comment">#将target进行一些变换[不需要]</span></span><br><span class="line">            target = self.target_transform(target)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> img, target<span class="comment">#返回数据和标签</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)</span><br><span class="line"></span><br><span class="line">dl_obj = MNIST_truncated</span><br><span class="line">train_dl_list = []</span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> net_dataidx_map.values():</span><br><span class="line">    train_dataset = dl_obj(args.datadir, dataidxs=idx, train=<span class="literal">True</span>, download=<span class="literal">False</span>, transform=transformer)</span><br><span class="line">    train_dl_list.append(DataLoader(dataset=train_dataset, batch_size=args.batchsize, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">False</span>))</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(dataset=train_set,batch_size=args.batchsize,shuffle=<span class="literal">True</span>)</span><br><span class="line">train_loader = DataLoader(dataset=train_dataset, batch_size=args.batchsize, shuffle=<span class="literal">True</span>, drop_last=<span class="literal">False</span>)</span><br><span class="line">test_loader = DataLoader(dataset=test_set,batch_size=args.batchsize,shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Client</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, args, idx, model, train_dl_local=<span class="literal">None</span>, test_dl_local=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.idx = idx</span><br><span class="line">        self.model = model</span><br><span class="line">        self.local_bs = args.train_local_batchsize</span><br><span class="line">        self.local_ep = args.train_local_epochs</span><br><span class="line">        self.lr = args.train_local_learnrate</span><br><span class="line">        self.device = args.device</span><br><span class="line">        self.loss_func = nn.CrossEntropyLoss()</span><br><span class="line">        self.ldr_train = train_dl_local</span><br><span class="line">        self.ldr_test = test_dl_local</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_state_dict</span>(<span class="params">self</span>):</span> <span class="comment"># 获取模型参数</span></span><br><span class="line">        <span class="keyword">return</span> self.model.state_dict()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_state_dict</span>(<span class="params">self, state_dict</span>):</span> <span class="comment"># 设置权重</span></span><br><span class="line">        self.model.load_state_dict(state_dict)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.model.train()</span><br><span class="line">        optimizer = optim.Adam(self.model.parameters(), lr=args.train_local_learnrate, weight_decay=<span class="number">0</span>)</span><br><span class="line">        epoch_loss = []</span><br><span class="line">        <span class="keyword">for</span> iteration <span class="keyword">in</span> <span class="built_in">range</span>(self.local_ep): <span class="comment"># 每个本地epoch</span></span><br><span class="line">            train_loss = <span class="number">0</span></span><br><span class="line">            cnt = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> batch_idx, (data, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.ldr_train):</span><br><span class="line">                <span class="comment"># data,labels = data.to(self.device),labels.to(self.device)</span></span><br><span class="line">                optimizer.zero_grad() <span class="comment">#使用optim.zero_grad()手动将梯度设置为零，因为PyTorch在默认情况下会累积梯度</span></span><br><span class="line">                output = self.model(data) <span class="comment">#生成网络的输出(前向传递)</span></span><br><span class="line">                loss = F.nll_loss(output, labels) <span class="comment">#计算输出（output）与真值标签（target）之间的负对数概率损失</span></span><br><span class="line">                </span><br><span class="line">                loss.backward() <span class="comment">#对损失反向传播</span></span><br><span class="line">                train_loss += loss.item() <span class="comment"># 加入</span></span><br><span class="line">                </span><br><span class="line">                optimizer.step() <span class="comment">#收集一组新的梯度，并使用optim.step()将其传播回每个网络参数</span></span><br><span class="line">                </span><br><span class="line">                cnt += labels.size(<span class="number">0</span>)</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># if batch_idx % 10 == 0:</span></span><br><span class="line">                    <span class="comment"># print(&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\tLoss: &#123;:.6f&#125;&#x27;.format(iteration, batch_idx * len(data), len(self.ldr_train.dataset),100. * batch_idx / len(self.ldr_train), loss.item()))</span></span><br><span class="line">                    <span class="comment">#神经网络模块以及优化器能够使用.state_dict()保存和加载它们的内部状态。这样，如果需要，我们就可以继续从以前保存的状态dict中进行训练——只需调用.load_state_dict(state_dict)。</span></span><br><span class="line">                    <span class="comment"># torch.save(self.model.state_dict(), &#x27;./model.pth&#x27;)</span></span><br><span class="line">                    <span class="comment"># torch.save(optimzer.state_dict(), &#x27;./optim.pth&#x27;)</span></span><br><span class="line">            epoch_loss.append(train_loss / cnt)</span><br><span class="line">        <span class="keyword">return</span> np.<span class="built_in">round</span>(<span class="built_in">sum</span>(epoch_loss)/<span class="built_in">len</span>(epoch_loss), <span class="number">5</span>) <span class="comment"># 返回四舍五入均值loss</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">eval_test</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">        test_loss = <span class="number">0</span></span><br><span class="line">        test_acc = <span class="number">0</span></span><br><span class="line">        cnt = <span class="number">0</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad(): <span class="comment">#使用上下文管理器no_grad()，我们可以避免将生成网络输出的计算结果存储在计算图（计算过程的构建，以便梯度反向传播等操作）中。（with是使用的意思）</span></span><br><span class="line">            <span class="keyword">for</span> data, labels <span class="keyword">in</span> self.ldr_test:</span><br><span class="line">                output = self.model(data) <span class="comment">#生成网络的输出(前向传递)</span></span><br><span class="line">                loss = F.nll_loss(output, labels) <span class="comment">#计算输出（output）与真值标签（target）之间的负对数概率损失</span></span><br><span class="line">                test_loss += loss.item()</span><br><span class="line">                pred = output.data.<span class="built_in">max</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>] <span class="comment">## 找到概率最大的下标</span></span><br><span class="line">                test_acc += pred.eq(labels.data.view_as(pred)).<span class="built_in">sum</span>() <span class="comment">#预测正确的数量相加</span></span><br><span class="line">                cnt += labels.size(<span class="number">0</span>)</span><br><span class="line">        test_loss /= <span class="built_in">len</span>(test_loader.dataset)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\nTest set: Avg. loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\n&#x27;</span>.<span class="built_in">format</span>(test_loss, test_acc, <span class="built_in">len</span>(self.ldr_test.dataset),<span class="number">100.</span> * test_acc / <span class="built_in">len</span>(self.ldr_test.dataset)))</span><br><span class="line">        <span class="built_in">print</span>(np.<span class="built_in">round</span>(<span class="number">100</span>*test_acc / cnt))</span><br><span class="line">        <span class="keyword">return</span> np.<span class="built_in">round</span>(<span class="number">100</span>*test_acc / cnt), np.<span class="built_in">round</span>(test_loss / cnt, <span class="number">5</span>) <span class="comment"># mark</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__== <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    clients = []</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(args.num_clients):</span><br><span class="line">        clients.append(Client(args, idx, copy.deepcopy(net_glob), train_loader, test_loader))</span><br><span class="line">    global_acc, global_loss = [], []</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">iter</span> <span class="keyword">in</span> <span class="built_in">range</span>(args.epoch):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;################################## ROUND <span class="subst">&#123;<span class="built_in">iter</span> + <span class="number">1</span>&#125;</span> ##################################&#x27;</span>)</span><br><span class="line">        m = <span class="built_in">max</span>(<span class="built_in">int</span>(args.work_frac * args.num_clients), <span class="number">1</span>) <span class="comment"># 每轮参与的比例(默认全部参加)*客户端数 step2</span></span><br><span class="line">        idxs_workers = np.random.choice(<span class="built_in">range</span>(args.num_clients), m, replace=<span class="literal">False</span>)  <span class="comment"># 随机选每轮客户端 step3</span></span><br><span class="line">        </span><br><span class="line">        w_locals, loss_locals = [], []  <span class="comment"># 每个客户端训练完的模型参数， 训练loss</span></span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> idxs_workers:</span><br><span class="line">            loss_train = clients[idx].train() <span class="comment"># step10</span></span><br><span class="line">            acc_be, loss_be = clients[idx].eval_test()</span><br><span class="line">            w_locals.append(copy.deepcopy(clients[idx].get_state_dict())) <span class="comment"># 模型参数</span></span><br><span class="line">            loss_locals.append(copy.deepcopy(loss_train)) <span class="comment"># 训练loss</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Local training completed, aggregation begins&quot;</span>)</span><br><span class="line">        weight_avg = [<span class="number">1</span> / <span class="built_in">len</span>(w_locals) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(w_locals))]</span><br><span class="line"></span><br><span class="line">        w_avg = copy.deepcopy(w_locals[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> w_avg.keys():</span><br><span class="line">            w_avg[k] = w_avg[k].cuda() * weight_avg[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> w_avg.keys(): <span class="comment"># 每个模型参数</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(w_locals)): <span class="comment"># 每个模型</span></span><br><span class="line">                w_avg[k] = w_avg[k].cuda() + w_locals[i][k].cuda() * weight_avg[i]</span><br><span class="line">        w_glob = w_avg <span class="comment"># 将所有模型参数传入得到新的模型参数</span></span><br><span class="line">        net_glob.load_state_dict(w_glob)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;aggregation done,global test begins&quot;</span>)</span><br><span class="line">        net_glob.<span class="built_in">eval</span>()</span><br><span class="line">        test_loss = <span class="number">0</span></span><br><span class="line">        test_acc = <span class="number">0</span></span><br><span class="line">        cnt = <span class="number">0</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad(): <span class="comment">#使用上下文管理器no_grad()，我们可以避免将生成网络输出的计算结果存储在计算图（计算过程的构建，以便梯度反向传播等操作）中。（with是使用的意思）</span></span><br><span class="line">            <span class="keyword">for</span> data, labels <span class="keyword">in</span> test_loader:</span><br><span class="line">                output = net_glob(data) <span class="comment">#生成网络的输出(前向传递)</span></span><br><span class="line">                loss = F.nll_loss(output, labels) <span class="comment">#计算输出（output）与真值标签（target）之间的负对数概率损失</span></span><br><span class="line">                test_loss += loss.item()</span><br><span class="line">                <span class="built_in">print</span>(output)</span><br><span class="line">                output1 = F.softmax(output) <span class="comment"># batch*10*24*24 -&gt; batch*10*12*12（2*2的池化层会减半，步长为2）（激活函数ReLU不改变形状）</span></span><br><span class="line"></span><br><span class="line">                pred = output.data.<span class="built_in">max</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>] <span class="comment">## 找到概率最大的下标</span></span><br><span class="line">                test_acc += pred.eq(labels.data.view_as(pred)).<span class="built_in">sum</span>() <span class="comment">#预测正确的数量相加</span></span><br><span class="line">                cnt += labels.size(<span class="number">0</span>)</span><br><span class="line">        test_loss /= <span class="built_in">len</span>(test_loader.dataset)</span><br><span class="line">        global_acc.append((<span class="number">100.</span> * test_acc / <span class="built_in">len</span>(test_loader.dataset)).item())</span><br><span class="line">        global_loss.append(test_loss)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\nTest set: Avg. loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\n&#x27;</span>.<span class="built_in">format</span>(test_loss, test_acc, <span class="built_in">len</span>(test_loader.dataset),<span class="number">100.</span> * test_acc / <span class="built_in">len</span>(test_loader.dataset)))</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;test done,next round begins&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(clients)):</span><br><span class="line">            clients[idx].set_state_dict(copy.deepcopy(w_glob))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Global acc: <span class="subst">&#123;global_acc&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Global los: <span class="subst">&#123;global_loss&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">    <span class="keyword">import</span> os</span><br><span class="line">    os.environ[<span class="string">&#x27;KMP_DUPLICATE_LIB_OK&#x27;</span>] = <span class="string">&#x27;TRUE&#x27;</span></span><br><span class="line">    x_plot = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(args.epoch)]</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    plt.plot(x_plot, global_acc)</span><br><span class="line">    plt.title(<span class="string">&quot;Accuracy&quot;</span>)</span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    plt.plot(x_plot, global_loss)</span><br><span class="line">    plt.title(<span class="string">&quot;Loss&quot;</span>)</span><br><span class="line">    plt.suptitle(<span class="string">f&quot;<span class="subst">&#123;args.dataset&#125;</span>_<span class="subst">&#123;args.model&#125;</span>_<span class="subst">&#123;args.partition&#125;</span>_<span class="subst">&#123;args.num_clients&#125;</span>_<span class="subst">&#123;args.work_frac&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="comment"># plt.savefig(&quot;.png&quot;)</span></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>


      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          
            <a href="/tags/%E8%AE%BA%E6%96%87/" rel="tag"># 论文</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2023/02/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/channel/FLIS%20noniid%E4%B8%8B%E8%81%9A%E7%B1%BB%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/" rel="next" title="FLIS 推理相似度聚类FL">
                <i class="fa fa-chevron-left"></i> FLIS 推理相似度聚类FL
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2023/02/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/channel/%E6%9C%80%E7%9B%B8%E5%85%B3channel%E4%BF%AE%E5%89%AA%E8%81%94%E9%82%A6%E9%81%97%E5%BF%98%E5%AD%A6%E4%B9%A0/" rel="prev" title="最相关channel剪枝联邦遗忘学习">
                最相关channel剪枝联邦遗忘学习 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="" />
          <p class="site-author-name" itemprop="name"></p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">104</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">15</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8A%A0%E8%BD%BD%E5%BA%93-%E5%8F%82%E6%95%B0%E5%AE%9A%E4%B9%89-argprase"><span class="nav-number">1.</span> <span class="nav-text">加载库,参数定义 argprase</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD-amp-%E5%A4%84%E7%90%86-transform-datasets-dataloader"><span class="nav-number">2.</span> <span class="nav-text">数据加载&amp;处理 transform datasets dataloader</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E5%AE%9A%E4%B9%89-nn-Module-torchvision-datasets"><span class="nav-number">3.</span> <span class="nav-text">网络定义: nn.Module &#x2F;torchvision.datasets</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E5%AE%9A%E4%B9%89-optim"><span class="nav-number">4.</span> <span class="nav-text">优化定义 optim</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83-train"><span class="nav-number">5.</span> <span class="nav-text">训练 train</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95-test"><span class="nav-number">6.</span> <span class="nav-text">测试 test</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BB%E5%87%BD%E6%95%B0"><span class="nav-number">7.</span> <span class="nav-text">主函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98"><span class="nav-number">8.</span> <span class="nav-text">模型保存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8"><span class="nav-number">9.</span> <span class="nav-text">&#x2F;&#x2F;模型使用</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81"><span class="nav-number"></span> <span class="nav-text">完整代码</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%812-%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="nav-number"></span> <span class="nav-text">完整代码2(可视化)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%81%94%E9%82%A6%E4%BB%A3%E7%A0%81"><span class="nav-number"></span> <span class="nav-text">联邦代码</span></a></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">hzlg</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" target="_blank" rel="noopener" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" target="_blank" rel="noopener" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  






  





  

  

  

  

  

  

</body>
</html>
